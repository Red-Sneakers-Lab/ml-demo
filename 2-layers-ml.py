# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G_3mxA5hBAS1foZ-EkeTEZ_6rnm2gpq5
"""

import numpy as np

# Сигмоида https://ru.wikipedia.org/wiki/%D0%A1%D0%B8%D0%B3%D0%BC%D0%BE%D0%B8%D0%B4%D0%B0
def nonlin(x,deriv=False):
    if(deriv==True):
        return nonlin(x)*(1-nonlin(x))
    return 1/(1+np.exp(-x))
    
# набор входных данных (массив это набор из 4х чисел среднее арифметическое которых мы ищем)
# вместе они образубт матрицу которую легко умножать на коефициенты (должны быть 0.5 для каждого числа) что б получить матрицу ответов
data = np.array([  
                [1, 1, 1],
                [1, 1, 0],
                [0, 0, 1],
                [0, 0, 0],
                [0, 1, 1]
                ])
    
# выходные данные
# матрица ответов (.T - Транспонирование)
result = np.array([[1, 1, 0, 0, 1]]).T

# сделаем случайные числа более определёнными
np.random.seed(1)

# инициализируем веса случайным образом
# матрица 4 * 1 с значениями [0, 1)
factor0 = np.random.random((3, 5))
factor1 = np.random.random((5, 1))

for iter in range(10000):
    layer0 = data
    layer1 = np.dot(layer0, factor0)
    layer2 = np.dot(layer1, factor1)

    error_layer2 = result - layer2    
    delta_layer2 = error_layer2 * nonlin(layer2, True)

    # вклад ошибки первого слоя в ошибку второго
    error_layer1 = np.dot(delta_layer2, factor1.T)
    delta_layer1 = error_layer1 * nonlin(layer1, True);

    factor0 += np.dot(layer0.T, delta_layer1)
    factor1 += np.dot(layer1.T, delta_layer2)

print("Коефициенты после тренировки:")
print(layer2)

